{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "l8kFb2C2zkI-",
        "outputId": "7098031f-c421-4e75-d489-e34c2d138c6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natural language processing is fun!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "text = \"Natural language Processing is Fun!\"\n",
        "text = text.lower()\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello world! NLP is exciting.\"\n",
        "text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "text # Hello world NLP is exciting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pz35yOmFz9OY",
        "outputId": "3bb8ad26-0b37-4a58-b353-50e2b9170590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello world NLP is exciting'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Tokenize this sentence.\"\n",
        "tokens = word_tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq70B1oX1qC2",
        "outputId": "ea2d8773-8d0e-4cbf-82fd-a636090e7da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenize', 'this', 'sentence', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [word for word in tokens if word.lower() not in stop_words]\n",
        "filtered"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4AOJsJy3Nrf",
        "outputId": "4c9bd72d-2abf-47c0-e912-156de24923bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenize', 'sentence', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"playing\", \"played\", \"player\"]\n",
        "stemmed = [stemmer.stem(word) for word in words]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m69J8Za-3qob",
        "outputId": "ab7b3b7a-eed3-4800-cbbc-967d1f767283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'player']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"playing\", \"better\", \"running\"]\n",
        "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "print(lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aROO69aI3wJw",
        "outputId": "69dc23b8-1b36-445d-904d-d7c66dbab496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['playing', 'good', 'running']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Part B*"
      ],
      "metadata": {
        "id": "SdQ8Fri54QpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Natural Language Processing (NLP) is transforming the way humans interact with machines.\""
      ],
      "metadata": {
        "id": "tzlajf_g4awF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = sentence.lower()\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "k-d-dcTY4zl-",
        "outputId": "fd7e9151-3417-4dc7-8757-b8bae213bfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natural language processing (nlp) is transforming the way humans interact with machines.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = sentence.translate(str.maketrans('','', string.punctuation))\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IAJXqrM747WT",
        "outputId": "921a09d8-c768-46b3-fd12-6a3c95b8f955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natural language processing nlp is transforming the way humans interact with machines'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_tokens = word_tokenize(sentence)\n",
        "s_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfnHhm756a6p",
        "outputId": "507ed514-b653-4205-b5e5-4b3d1313f0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'nlp',\n",
              " 'is',\n",
              " 'transforming',\n",
              " 'the',\n",
              " 'way',\n",
              " 'humans',\n",
              " 'interact',\n",
              " 'with',\n",
              " 'machines']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words_rid_of_sentence = [word for word in s_tokens if word.lower() not in stop_words]\n",
        "stop_words_rid_of_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhCV_8U76xwn",
        "outputId": "ec14fec8-81da-423f-dfa8-fa46bae9ac0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'nlp',\n",
              " 'transforming',\n",
              " 'way',\n",
              " 'humans',\n",
              " 'interact',\n",
              " 'machines']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_of_sentence = [stemmer.stem(stop_word_rid_of_sentence) for stop_word_rid_of_sentence in stop_words_rid_of_sentence]\n",
        "stemmed_of_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtXpdCTk7cP0",
        "outputId": "a3b48b17-3b56-4936-b650-3654d226220e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natur',\n",
              " 'languag',\n",
              " 'process',\n",
              " 'nlp',\n",
              " 'transform',\n",
              " 'way',\n",
              " 'human',\n",
              " 'interact',\n",
              " 'machin']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_of_sentence = [lemmatizer.lemmatize(stem_of_sentence, pos='v') for stem_of_sentence in stemmed_of_sentence]\n",
        "lemmatized_of_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02JwAMtU8AQV",
        "outputId": "a1644695-494a-4f21-a439-fe0484d85942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natur',\n",
              " 'languag',\n",
              " 'process',\n",
              " 'nlp',\n",
              " 'transform',\n",
              " 'way',\n",
              " 'human',\n",
              " 'interact',\n",
              " 'machin']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The running dogs were quickly chasing the beautiful cats through the gardens..!\""
      ],
      "metadata": {
        "id": "8H5o-yHmZ_PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== TOKENIZATION ===\")\n",
        "# Breaking text into individual words (tokens)\n",
        "tokens = word_tokenize(text)\n",
        "print(f\"Original: {text}\")\n",
        "print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fIMv4bcaAl6",
        "outputId": "b00e1a7a-7fdd-46b0-d55a-6e8cb961ab5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TOKENIZATION ===\n",
            "Original: The running dogs were quickly chasing the beautiful cats through the gardens..!\n",
            "Tokens: ['The', 'running', 'dogs', 'were', 'quickly', 'chasing', 'the', 'beautiful', 'cats', 'through', 'the', 'gardens', '..', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== STOPWORD REMOVAL ===\")\n",
        "# Remove common words that don't carry much meaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(f\"Stop words: {list(stop_words)[:10]}...\")  # Show first 10\n",
        "print(f\"After removing stop words: {filtered_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vAQkSThaFzQ",
        "outputId": "ddcfdec7-3912-48b9-d186-b1d008a24004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STOPWORD REMOVAL ===\n",
            "Stop words: [\"we're\", 'while', 'mightn', 'weren', \"i'm\", 'very', 'can', 'after', \"he'll\", 'how']...\n",
            "After removing stop words: ['running', 'dogs', 'quickly', 'chasing', 'beautiful', 'cats', 'gardens', '..', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== STEMMING vs LEMMATIZATION ===\")\n",
        "# Stemming: crude chopping to find root form\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(f\"Stemmed words: {stemmed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNA1CRDoapcu",
        "outputId": "d6f07c74-5d8c-4834-82fc-fab2d8633b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STEMMING vs LEMMATIZATION ===\n",
            "Stemmed words: ['run', 'dog', 'quickli', 'chase', 'beauti', 'cat', 'garden', '..', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization: intelligent reduction to dictionary form\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in filtered_tokens]\n",
        "print(f\"Lemmatized words: {lemmatized}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO4cKw1HbBRS",
        "outputId": "214ca4a6-9be3-46eb-f364-19037675f4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['run', 'dog', 'quickly', 'chase', 'beautiful', 'cat', 'garden', '..', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "print(\"=== N-GRAMS ===\")\n",
        "# Creating sequences of n consecutive words\n",
        "bigrams = list(ngrams(tokens, 2))  # 2-word combinations\n",
        "trigrams = list(ngrams(tokens, 3))  # 3-word combinations\n",
        "print(f\"Bigrams (2-word sequences): {bigrams}\")\n",
        "print(f\"Trigrams (3-word sequences): {trigrams}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGulchFzbTJu",
        "outputId": "bd742a7c-2493-4e3f-8777-66fa3ae3ac68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== N-GRAMS ===\n",
            "Bigrams (2-word sequences): [('The', 'running'), ('running', 'dogs'), ('dogs', 'were'), ('were', 'quickly'), ('quickly', 'chasing'), ('chasing', 'the'), ('the', 'beautiful'), ('beautiful', 'cats'), ('cats', 'through'), ('through', 'the'), ('the', 'gardens'), ('gardens', '..'), ('..', '!')]\n",
            "Trigrams (3-word sequences): [('The', 'running', 'dogs'), ('running', 'dogs', 'were'), ('dogs', 'were', 'quickly'), ('were', 'quickly', 'chasing'), ('quickly', 'chasing', 'the'), ('chasing', 'the', 'beautiful'), ('the', 'beautiful', 'cats'), ('beautiful', 'cats', 'through'), ('cats', 'through', 'the'), ('through', 'the', 'gardens'), ('the', 'gardens', '..'), ('gardens', '..', '!')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "!pip install python-Levenshtein\n",
        "from Levenshtein import distance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7swQ0Zocn9W",
        "outputId": "f72cd2ad-9f70-4b62-a582-60c202d9e6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== STRING DISTANCE METRICS ===\")\n",
        "word1, word2 = \"running\", \"runing\"  # Intentional typo\n",
        "word3 = \"walking\"\n",
        "\n",
        "# Edit distance: minimum operations to transform one string to another\n",
        "nltk_distance = edit_distance(word1, word2)\n",
        "levenshtein_dist = distance(word1, word2)\n",
        "different_words_dist = distance(word1, word3)\n",
        "\n",
        "print(f\"Distance between '{word1}' and '{word2}': {nltk_distance} (NLTK)\")\n",
        "print(f\"Distance between '{word1}' and '{word2}': {levenshtein_dist} (Levenshtein)\")\n",
        "print(f\"Distance between '{word1}' and '{word3}': {different_words_dist}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIi1IM-Ob4yB",
        "outputId": "97e96184-8ea3-4b2f-d488-95852b2194a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STRING DISTANCE METRICS ===\n",
            "Distance between 'running' and 'runing': 1 (NLTK)\n",
            "Distance between 'running' and 'runing': 1 (Levenshtein)\n",
            "Distance between 'running' and 'walking': 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_pos_in_lemmatization():\n",
        "    \"\"\"\n",
        "    Demonstrates how part-of-speech (POS) tags affect lemmatization results.\n",
        "    This is crucial for understanding why context matters in NLP.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create sample sentences with words that behave differently as different parts of speech\n",
        "    test_words = [\n",
        "        (\"running\", \"I am running fast\"),           # verb vs adjective/noun\n",
        "        (\"better\", \"This is better than that\"),    # adjective vs adverb\n",
        "        (\"studies\", \"She studies hard\"),           # verb vs noun\n",
        "        (\"leaves\", \"The tree leaves in autumn\"),   # verb vs noun\n",
        "        (\"flies\", \"Time flies quickly\")            # verb vs noun\n",
        "    ]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    print(\"=== LEMMATIZATION WITHOUT POS (Default Behavior) ===\")\n",
        "    print(\"When you don't specify POS, WordNet assumes the word is a NOUN by default\")\n",
        "    print(\"This is like assuming every word you encounter is a person's name!\\n\")\n",
        "\n",
        "    for word, context in test_words:\n",
        "        # Default lemmatization (assumes noun)\n",
        "        default_result = lemmatizer.lemmatize(word)\n",
        "        print(f\"Word: '{word}' (in context: '{context}')\")\n",
        "        print(f\"  Default lemmatization (assumes noun): '{word}' → '{default_result}'\")\n",
        "\n",
        "        # Now try different POS tags to see the difference\n",
        "        as_noun = lemmatizer.lemmatize(word, pos='n')      # noun\n",
        "        as_verb = lemmatizer.lemmatize(word, pos='v')      # verb\n",
        "        as_adj = lemmatizer.lemmatize(word, pos='a')       # adjective\n",
        "        as_adv = lemmatizer.lemmatize(word, pos='r')       # adverb\n",
        "\n",
        "        print(f\"  As noun (pos='n'):      '{word}' → '{as_noun}'\")\n",
        "        print(f\"  As verb (pos='v'):      '{word}' → '{as_verb}'\")\n",
        "        print(f\"  As adjective (pos='a'): '{word}' → '{as_adj}'\")\n",
        "        print(f\"  As adverb (pos='r'):    '{word}' → '{as_adv}'\")\n",
        "        print()\n",
        "\n",
        "    print(\"\\n=== WHY THIS MATTERS: A CONCRETE EXAMPLE ===\")\n",
        "\n",
        "    # Show how context completely changes meaning and lemmatization\n",
        "    sentence1 = \"The running water was cold\"      # \"running\" is an adjective here\n",
        "    sentence2 = \"I was running to the store\"     # \"running\" is a verb here\n",
        "\n",
        "    print(f\"Sentence 1: '{sentence1}'\")\n",
        "    print(f\"  'running' as adjective: '{lemmatizer.lemmatize('running', pos='a')}'\")\n",
        "    print(f\"  'running' as verb: '{lemmatizer.lemmatize('running', pos='v')}'\")\n",
        "    print(f\"  'running' without POS: '{lemmatizer.lemmatize('running')}'\")\n",
        "\n",
        "    print(f\"\\nSentence 2: '{sentence2}'\")\n",
        "    print(\"In this context, 'running' should be lemmatized as a verb!\")\n",
        "    print(f\"  Correct lemmatization (verb): '{lemmatizer.lemmatize('running', pos='v')}'\")\n",
        "    print(f\"  Wrong assumption (default): '{lemmatizer.lemmatize('running')}'\")\n",
        "\n",
        "    print(\"\\n=== PRACTICAL IMPLICATIONS ===\")\n",
        "\n",
        "    # Demonstrate how this affects information retrieval\n",
        "    documents = [\n",
        "        \"The company studies market trends carefully\",     # \"studies\" = verb\n",
        "        \"Recent studies show interesting patterns\",        # \"studies\" = noun\n",
        "        \"She studies computer science at university\"       # \"studies\" = verb\n",
        "    ]\n",
        "\n",
        "    print(\"Consider these documents in a search engine:\")\n",
        "    for i, doc in enumerate(documents, 1):\n",
        "        tokens = word_tokenize(doc.lower())\n",
        "        studies_word = \"studies\"\n",
        "\n",
        "        # Find the word \"studies\" in each document\n",
        "        if studies_word in tokens:\n",
        "            default_lemma = lemmatizer.lemmatize(studies_word)\n",
        "            verb_lemma = lemmatizer.lemmatize(studies_word, pos='v')\n",
        "            noun_lemma = lemmatizer.lemmatize(studies_word, pos='n')\n",
        "\n",
        "            print(f\"\\nDocument {i}: '{doc}'\")\n",
        "            print(f\"  Default lemmatization: '{studies_word}' → '{default_lemma}'\")\n",
        "            print(f\"  As verb: '{studies_word}' → '{verb_lemma}'\")\n",
        "            print(f\"  As noun: '{studies_word}' → '{noun_lemma}'\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"KEY INSIGHTS:\")\n",
        "    print(\"• Without POS tags, lemmatization defaults to treating words as nouns\")\n",
        "    print(\"• This can lead to missed connections between related concepts\")\n",
        "    print(\"• In information retrieval, you might miss relevant documents\")\n",
        "    print(\"• For better accuracy, you need POS tagging before lemmatization\")\n",
        "    print(\"• This is why advanced NLP pipelines always include POS tagging!\")\n",
        "\n",
        "def show_pos_tagging_integration():\n",
        "    \"\"\"\n",
        "    Shows how to properly integrate POS tagging with lemmatization\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"PROPER INTEGRATION: POS TAGGING + LEMMATIZATION\")\n",
        "\n",
        "    # First, we need to download POS tagger\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "    text = \"The running dogs were quickly studying the flying birds\"\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Get POS tags for each word\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    print(f\"Original text: '{text}'\")\n",
        "    print(f\"Tokens with POS tags: {pos_tags}\")\n",
        "\n",
        "    # Convert NLTK POS tags to WordNet POS tags\n",
        "    def get_wordnet_pos(treebank_tag):\n",
        "        \"\"\"\n",
        "        Converts NLTK's POS tags to WordNet's POS tags\n",
        "        This is like translating between two different classification systems\n",
        "        \"\"\"\n",
        "        if treebank_tag.startswith('J'):\n",
        "            return 'a'  # adjective\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return 'v'  # verb\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return 'n'  # noun\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return 'r'  # adverb\n",
        "        else:\n",
        "            return 'n'  # default to noun\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    print(f\"\\nIntelligent lemmatization using POS tags:\")\n",
        "    for word, pos_tag in pos_tags:\n",
        "        if word.isalpha():  # Only process alphabetic words\n",
        "            wordnet_pos = get_wordnet_pos(pos_tag)\n",
        "            lemmatized = lemmatizer.lemmatize(word.lower(), pos=wordnet_pos)\n",
        "\n",
        "            print(f\"  '{word}' (POS: {pos_tag} → {wordnet_pos}) → '{lemmatized}'\")"
      ],
      "metadata": {
        "id": "NrrVE0M0cbGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demonstrate_pos_in_lemmatization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udzAb1n4foSI",
        "outputId": "645c6e8f-4910-442d-d4b1-ac330cc8ad73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LEMMATIZATION WITHOUT POS (Default Behavior) ===\n",
            "When you don't specify POS, WordNet assumes the word is a NOUN by default\n",
            "This is like assuming every word you encounter is a person's name!\n",
            "\n",
            "Word: 'running' (in context: 'I am running fast')\n",
            "  Default lemmatization (assumes noun): 'running' → 'running'\n",
            "  As noun (pos='n'):      'running' → 'running'\n",
            "  As verb (pos='v'):      'running' → 'run'\n",
            "  As adjective (pos='a'): 'running' → 'running'\n",
            "  As adverb (pos='r'):    'running' → 'running'\n",
            "\n",
            "Word: 'better' (in context: 'This is better than that')\n",
            "  Default lemmatization (assumes noun): 'better' → 'better'\n",
            "  As noun (pos='n'):      'better' → 'better'\n",
            "  As verb (pos='v'):      'better' → 'better'\n",
            "  As adjective (pos='a'): 'better' → 'good'\n",
            "  As adverb (pos='r'):    'better' → 'well'\n",
            "\n",
            "Word: 'studies' (in context: 'She studies hard')\n",
            "  Default lemmatization (assumes noun): 'studies' → 'study'\n",
            "  As noun (pos='n'):      'studies' → 'study'\n",
            "  As verb (pos='v'):      'studies' → 'study'\n",
            "  As adjective (pos='a'): 'studies' → 'studies'\n",
            "  As adverb (pos='r'):    'studies' → 'studies'\n",
            "\n",
            "Word: 'leaves' (in context: 'The tree leaves in autumn')\n",
            "  Default lemmatization (assumes noun): 'leaves' → 'leaf'\n",
            "  As noun (pos='n'):      'leaves' → 'leaf'\n",
            "  As verb (pos='v'):      'leaves' → 'leave'\n",
            "  As adjective (pos='a'): 'leaves' → 'leaves'\n",
            "  As adverb (pos='r'):    'leaves' → 'leaves'\n",
            "\n",
            "Word: 'flies' (in context: 'Time flies quickly')\n",
            "  Default lemmatization (assumes noun): 'flies' → 'fly'\n",
            "  As noun (pos='n'):      'flies' → 'fly'\n",
            "  As verb (pos='v'):      'flies' → 'fly'\n",
            "  As adjective (pos='a'): 'flies' → 'flies'\n",
            "  As adverb (pos='r'):    'flies' → 'flies'\n",
            "\n",
            "\n",
            "=== WHY THIS MATTERS: A CONCRETE EXAMPLE ===\n",
            "Sentence 1: 'The running water was cold'\n",
            "  'running' as adjective: 'running'\n",
            "  'running' as verb: 'run'\n",
            "  'running' without POS: 'running'\n",
            "\n",
            "Sentence 2: 'I was running to the store'\n",
            "In this context, 'running' should be lemmatized as a verb!\n",
            "  Correct lemmatization (verb): 'run'\n",
            "  Wrong assumption (default): 'running'\n",
            "\n",
            "=== PRACTICAL IMPLICATIONS ===\n",
            "Consider these documents in a search engine:\n",
            "\n",
            "Document 1: 'The company studies market trends carefully'\n",
            "  Default lemmatization: 'studies' → 'study'\n",
            "  As verb: 'studies' → 'study'\n",
            "  As noun: 'studies' → 'study'\n",
            "\n",
            "Document 2: 'Recent studies show interesting patterns'\n",
            "  Default lemmatization: 'studies' → 'study'\n",
            "  As verb: 'studies' → 'study'\n",
            "  As noun: 'studies' → 'study'\n",
            "\n",
            "Document 3: 'She studies computer science at university'\n",
            "  Default lemmatization: 'studies' → 'study'\n",
            "  As verb: 'studies' → 'study'\n",
            "  As noun: 'studies' → 'study'\n",
            "\n",
            "============================================================\n",
            "KEY INSIGHTS:\n",
            "• Without POS tags, lemmatization defaults to treating words as nouns\n",
            "• This can lead to missed connections between related concepts\n",
            "• In information retrieval, you might miss relevant documents\n",
            "• For better accuracy, you need POS tagging before lemmatization\n",
            "• This is why advanced NLP pipelines always include POS tagging!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def explore_punctuation_removal():\n",
        "    \"\"\"\n",
        "    Deep dive into how str.maketrans() and translate() work together\n",
        "    to remove punctuation from text efficiently.\n",
        "    \"\"\"\n",
        "\n",
        "    text = \"Hello world! NLP is exciting.\"\n",
        "\n",
        "    print(\"=== UNDERSTANDING THE COMPONENTS ===\\n\")\n",
        "\n",
        "    # First, let's see what string.punctuation contains\n",
        "    print(\"1. What is string.punctuation?\")\n",
        "    print(f\"   Contents: '{string.punctuation}'\")\n",
        "    print(f\"   These are all ASCII punctuation characters\")\n",
        "    print(f\"   Length: {len(string.punctuation)} characters\\n\")\n",
        "\n",
        "    # Now let's understand str.maketrans()\n",
        "    print(\"2. Understanding str.maketrans()\")\n",
        "    print(\"   This function creates a TRANSLATION TABLE - think of it as a dictionary\")\n",
        "    print(\"   that tells Python how to map characters from one set to another.\\n\")\n",
        "\n",
        "    # The three-argument form: maketrans(from, to, delete)\n",
        "    # maketrans(\"\", \"\", string.punctuation) means:\n",
        "    # - from: \"\" (empty - don't replace anything)\n",
        "    # - to: \"\" (empty - don't replace with anything)\n",
        "    # - delete: string.punctuation (delete these characters)\n",
        "\n",
        "    translation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "    print(f\"   Translation table type: {type(translation_table)}\")\n",
        "    print(f\"   It's a dictionary that maps character codes to actions\")\n",
        "    print(f\"   Sample entries (first 5 punctuation marks):\")\n",
        "\n",
        "    # Show how the translation table works\n",
        "    for i, char in enumerate(string.punctuation[:5]):\n",
        "        char_code = ord(char)\n",
        "        print(f\"     '{char}' (code {char_code}) → None (means DELETE)\")\n",
        "\n",
        "    print(f\"\\n   Total mappings in table: {len(translation_table)}\\n\")\n",
        "\n",
        "    # Now let's see translate() in action step by step\n",
        "    print(\"3. How translate() applies the translation table\")\n",
        "    print(f\"   Original text: '{text}'\")\n",
        "    print(f\"   Processing character by character:\\n\")\n",
        "\n",
        "    # Manual simulation of what translate() does\n",
        "    result = \"\"\n",
        "    for i, char in enumerate(text):\n",
        "        char_code = ord(char)\n",
        "        if char_code in translation_table:\n",
        "            # Character is in translation table (it's punctuation)\n",
        "            print(f\"     Position {i}: '{char}' → DELETED (found in table)\")\n",
        "        else:\n",
        "            # Character is not in translation table (keep it)\n",
        "            result += char\n",
        "            print(f\"     Position {i}: '{char}' → KEPT\")\n",
        "\n",
        "    print(f\"\\n   Result after manual processing: '{result}'\")\n",
        "\n",
        "    # Now do it with the actual translate method\n",
        "    cleaned_text = text.translate(translation_table)\n",
        "    print(f\"   Result using translate(): '{cleaned_text}'\")\n",
        "    print(f\"   Both methods produce identical results: {result == cleaned_text}\\n\")\n",
        "\n",
        "    print(\"=== WHY USE THIS APPROACH? EFFICIENCY! ===\\n\")\n",
        "\n",
        "    # Compare different approaches to removing punctuation\n",
        "    import time\n",
        "\n",
        "    test_text = text * 1000  # Repeat text 1000 times\n",
        "\n",
        "    print(\"Let's compare three different methods on larger text:\\n\")\n",
        "\n",
        "    # Method 1: Using translate (the code you asked about)\n",
        "    start = time.time()\n",
        "    result1 = test_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    time1 = time.time() - start\n",
        "    print(f\"Method 1 - translate(): {time1:.6f} seconds\")\n",
        "    print(\"   Pros: Fastest, concise, happens at C level in Python\")\n",
        "    print(\"   Cons: Less intuitive at first glance\\n\")\n",
        "\n",
        "    # Method 2: Using list comprehension\n",
        "    start = time.time()\n",
        "    result2 = ''.join([char for char in test_text if char not in string.punctuation])\n",
        "    time2 = time.time() - start\n",
        "    print(f\"Method 2 - list comprehension: {time2:.6f} seconds\")\n",
        "    print(\"   Pros: More readable, Pythonic\")\n",
        "    print(\"   Cons: Slower, creates intermediate list\\n\")\n",
        "\n",
        "    # Method 3: Using replace in a loop\n",
        "    start = time.time()\n",
        "    result3 = test_text\n",
        "    for punct in string.punctuation:\n",
        "        result3 = result3.replace(punct, \"\")\n",
        "    time3 = time.time() - start\n",
        "    print(f\"Method 3 - multiple replace(): {time3:.6f} seconds\")\n",
        "    print(\"   Pros: Very intuitive, easy to understand\")\n",
        "    print(\"   Cons: Slowest, creates many intermediate strings\\n\")\n",
        "\n",
        "    print(f\"Speed comparison:\")\n",
        "    print(f\"   translate() is {time2/time1:.1f}x faster than list comprehension\")\n",
        "    print(f\"   translate() is {time3/time1:.1f}x faster than multiple replace()\\n\")\n",
        "\n",
        "    # Verify all methods produce the same result\n",
        "    print(f\"All methods produce identical output: {result1 == result2 == result3}\\n\")\n",
        "\n",
        "    print(\"=== ALTERNATIVE FORMS OF str.maketrans() ===\\n\")\n",
        "\n",
        "    # str.maketrans() can be used in different ways\n",
        "    print(\"The maketrans() method has multiple signatures:\\n\")\n",
        "\n",
        "    # One argument: dictionary mapping\n",
        "    print(\"1. Dictionary form (most flexible):\")\n",
        "    custom_table = str.maketrans({\n",
        "        '!': None,      # Delete exclamation marks\n",
        "        '.': None,      # Delete periods\n",
        "        'o': '0',       # Replace 'o' with '0'\n",
        "        'l': '1'        # Replace 'l' with '1'\n",
        "    })\n",
        "    print(f\"   Input:  '{text}'\")\n",
        "    print(f\"   Output: '{text.translate(custom_table)}'\")\n",
        "    print(\"   Used when you want fine control over specific characters\\n\")\n",
        "\n",
        "    # Two arguments: character-by-character replacement\n",
        "    print(\"2. Two-string form (character substitution):\")\n",
        "    vowel_table = str.maketrans(\"aeiou\", \"43!0*\")\n",
        "    sample = \"Hello Python\"\n",
        "    print(f\"   Input:  '{sample}'\")\n",
        "    print(f\"   Output: '{sample.translate(vowel_table)}'\")\n",
        "    print(\"   Each character in first string maps to corresponding character in second\\n\")\n",
        "\n",
        "    # Three arguments: replace and delete (what you're using)\n",
        "    print(\"3. Three-argument form (replace + delete):\")\n",
        "    print(\"   maketrans(from_chars, to_chars, delete_chars)\")\n",
        "    print(\"   Your code uses: maketrans('', '', string.punctuation)\")\n",
        "    print(\"   Meaning: replace nothing, delete all punctuation\\n\")\n",
        "\n",
        "    print(\"=== WHY THIS MATTERS IN NLP ===\\n\")\n",
        "\n",
        "    documents = [\n",
        "        \"Hello, world! How are you?\",\n",
        "        \"Machine learning is amazing!!!\",\n",
        "        \"Natural Language Processing (NLP) transforms text.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Consider these documents for text analysis:\")\n",
        "    for i, doc in enumerate(documents, 1):\n",
        "        clean = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "        tokens = clean.split()\n",
        "        print(f\"\\nDocument {i}: '{doc}'\")\n",
        "        print(f\"  Cleaned: '{clean}'\")\n",
        "        print(f\"  Tokens: {tokens}\")\n",
        "        print(f\"  Without punctuation removal, '!' and '?' would be tokens\")\n",
        "        print(f\"  This would pollute your vocabulary and skew word frequencies\")\n",
        "\n",
        "    print(\"\\n=== KEY TAKEAWAYS ===\")\n",
        "    print(\"• str.maketrans() creates a character mapping table (dictionary)\")\n",
        "    print(\"• translate() applies that table efficiently in one pass\")\n",
        "    print(\"• This is the fastest way to remove or replace characters in Python\")\n",
        "    print(\"• Essential for text preprocessing in NLP pipelines\")\n",
        "    print(\"• Removes noise (punctuation) to focus on meaningful content (words)\")"
      ],
      "metadata": {
        "id": "O3i4hlqefq5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explore_punctuation_removal()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC8QCSgsiSQm",
        "outputId": "6fd2721c-a5d4-41dc-b931-5a78067d280c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== UNDERSTANDING THE COMPONENTS ===\n",
            "\n",
            "1. What is string.punctuation?\n",
            "   Contents: '!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
            "   These are all ASCII punctuation characters\n",
            "   Length: 32 characters\n",
            "\n",
            "2. Understanding str.maketrans()\n",
            "   This function creates a TRANSLATION TABLE - think of it as a dictionary\n",
            "   that tells Python how to map characters from one set to another.\n",
            "\n",
            "   Translation table type: <class 'dict'>\n",
            "   It's a dictionary that maps character codes to actions\n",
            "   Sample entries (first 5 punctuation marks):\n",
            "     '!' (code 33) → None (means DELETE)\n",
            "     '\"' (code 34) → None (means DELETE)\n",
            "     '#' (code 35) → None (means DELETE)\n",
            "     '$' (code 36) → None (means DELETE)\n",
            "     '%' (code 37) → None (means DELETE)\n",
            "\n",
            "   Total mappings in table: 32\n",
            "\n",
            "3. How translate() applies the translation table\n",
            "   Original text: 'Hello world! NLP is exciting.'\n",
            "   Processing character by character:\n",
            "\n",
            "     Position 0: 'H' → KEPT\n",
            "     Position 1: 'e' → KEPT\n",
            "     Position 2: 'l' → KEPT\n",
            "     Position 3: 'l' → KEPT\n",
            "     Position 4: 'o' → KEPT\n",
            "     Position 5: ' ' → KEPT\n",
            "     Position 6: 'w' → KEPT\n",
            "     Position 7: 'o' → KEPT\n",
            "     Position 8: 'r' → KEPT\n",
            "     Position 9: 'l' → KEPT\n",
            "     Position 10: 'd' → KEPT\n",
            "     Position 11: '!' → DELETED (found in table)\n",
            "     Position 12: ' ' → KEPT\n",
            "     Position 13: 'N' → KEPT\n",
            "     Position 14: 'L' → KEPT\n",
            "     Position 15: 'P' → KEPT\n",
            "     Position 16: ' ' → KEPT\n",
            "     Position 17: 'i' → KEPT\n",
            "     Position 18: 's' → KEPT\n",
            "     Position 19: ' ' → KEPT\n",
            "     Position 20: 'e' → KEPT\n",
            "     Position 21: 'x' → KEPT\n",
            "     Position 22: 'c' → KEPT\n",
            "     Position 23: 'i' → KEPT\n",
            "     Position 24: 't' → KEPT\n",
            "     Position 25: 'i' → KEPT\n",
            "     Position 26: 'n' → KEPT\n",
            "     Position 27: 'g' → KEPT\n",
            "     Position 28: '.' → DELETED (found in table)\n",
            "\n",
            "   Result after manual processing: 'Hello world NLP is exciting'\n",
            "   Result using translate(): 'Hello world NLP is exciting'\n",
            "   Both methods produce identical results: True\n",
            "\n",
            "=== WHY USE THIS APPROACH? EFFICIENCY! ===\n",
            "\n",
            "Let's compare three different methods on larger text:\n",
            "\n",
            "Method 1 - translate(): 0.000064 seconds\n",
            "   Pros: Fastest, concise, happens at C level in Python\n",
            "   Cons: Less intuitive at first glance\n",
            "\n",
            "Method 2 - list comprehension: 0.001741 seconds\n",
            "   Pros: More readable, Pythonic\n",
            "   Cons: Slower, creates intermediate list\n",
            "\n",
            "Method 3 - multiple replace(): 0.000630 seconds\n",
            "   Pros: Very intuitive, easy to understand\n",
            "   Cons: Slowest, creates many intermediate strings\n",
            "\n",
            "Speed comparison:\n",
            "   translate() is 27.0x faster than list comprehension\n",
            "   translate() is 9.8x faster than multiple replace()\n",
            "\n",
            "All methods produce identical output: True\n",
            "\n",
            "=== ALTERNATIVE FORMS OF str.maketrans() ===\n",
            "\n",
            "The maketrans() method has multiple signatures:\n",
            "\n",
            "1. Dictionary form (most flexible):\n",
            "   Input:  'Hello world! NLP is exciting.'\n",
            "   Output: 'He110 w0r1d NLP is exciting'\n",
            "   Used when you want fine control over specific characters\n",
            "\n",
            "2. Two-string form (character substitution):\n",
            "   Input:  'Hello Python'\n",
            "   Output: 'H3ll0 Pyth0n'\n",
            "   Each character in first string maps to corresponding character in second\n",
            "\n",
            "3. Three-argument form (replace + delete):\n",
            "   maketrans(from_chars, to_chars, delete_chars)\n",
            "   Your code uses: maketrans('', '', string.punctuation)\n",
            "   Meaning: replace nothing, delete all punctuation\n",
            "\n",
            "=== WHY THIS MATTERS IN NLP ===\n",
            "\n",
            "Consider these documents for text analysis:\n",
            "\n",
            "Document 1: 'Hello, world! How are you?'\n",
            "  Cleaned: 'Hello world How are you'\n",
            "  Tokens: ['Hello', 'world', 'How', 'are', 'you']\n",
            "  Without punctuation removal, '!' and '?' would be tokens\n",
            "  This would pollute your vocabulary and skew word frequencies\n",
            "\n",
            "Document 2: 'Machine learning is amazing!!!'\n",
            "  Cleaned: 'Machine learning is amazing'\n",
            "  Tokens: ['Machine', 'learning', 'is', 'amazing']\n",
            "  Without punctuation removal, '!' and '?' would be tokens\n",
            "  This would pollute your vocabulary and skew word frequencies\n",
            "\n",
            "Document 3: 'Natural Language Processing (NLP) transforms text.'\n",
            "  Cleaned: 'Natural Language Processing NLP transforms text'\n",
            "  Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'transforms', 'text']\n",
            "  Without punctuation removal, '!' and '?' would be tokens\n",
            "  This would pollute your vocabulary and skew word frequencies\n",
            "\n",
            "=== KEY TAKEAWAYS ===\n",
            "• str.maketrans() creates a character mapping table (dictionary)\n",
            "• translate() applies that table efficiently in one pass\n",
            "• This is the fastest way to remove or replace characters in Python\n",
            "• Essential for text preprocessing in NLP pipelines\n",
            "• Removes noise (punctuation) to focus on meaningful content (words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2yBP4iHiWT4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}